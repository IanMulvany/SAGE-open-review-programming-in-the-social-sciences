{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scaling Up: Thinking about Programming in the Social Sciences\n",
    "\n",
    "I was pretty lucky as a child. I grew up in Gander, Newfoundland, Canada. It didn't realise it at the time, but Gander was kind of an experiment in social planning. Located at the eastern tip of North America, high on a plateau away from the stormy coasts, Gander was selected as the site of an intercontinental airstrip for refuelling flights between North America and Europe. During World War II, Gander was little more than the airport. In a few later decades when intercontinental travel became the norm, Gander was selected to be Canada's first international airport servicing flights between New York, Washington, or Toronto, with places like London, Paris, and Berlin. As a child I remember the constant flight traffic and airplane themes on everything from the names of the streets to the town festival (the festival of flight).\n",
    "\n",
    "One day I was given a map of the town and shown something that I find striking to this day. Three of the main streets encircling the town square, one of which I lived on, created the silhouette of the head of a goose (a male goose being called a 'gander'). Here, inscribed literally in the fabric of the town was the name and the town's symbol. I had played on these streets, walked, been driven, or cycled up and down Memorial Drive, Edinburgh Road, and Elizabeth Drive hundreds of times. Never would I have thought that these three streets together made the shape of a goose head. That this was intentional was a part of town lore, with maps highlighting the goose head for tourists. Yet, until I had seen a map or was old enough to read one, I did not realise it was the case -- for I wasn't looking at the right scale.\n",
    "\n",
    "Social science is often about considering things at scales that are meaningful, like the goose head, but difficult to comprehend through everyday experience, like walking around the streets. As an example, think about the newsfeed on Facebook or Twitter. Consider viewing the feed one story at a time, scrolling endlessly until you switch to another task. It's the level of human experience. Now, how was that feed determined? Which stories came first or second? Do you know why Twitter showed that politician followed by this specific friend followed by that advertisement? Do you know why Facebook decides which posts to place at the top or to bring back as memories many years later? Do you know how many advertisements users will tolerate in their feed? Has the number changed over time or does it change in different markets? The answers to these questions are partially knowable, but not at the level of simply reading a newsfeed like a normal user. Instead, a user would have to know how to make a claim with evidence...and also to know which evidence is the right kind of evidence. To do that sometimes we need to zoom out from everyday experience to see things at the right scale.\n",
    "\n",
    "Data science is a relatively new field in terms of disciplines. Later on, I will give a little bit of an elaborate definition of data science and _social_ data science. But for now, let's just say that data science seems to have emerged from a new set of opportunities: much of our world is now being recorded or mediated. This means that there exists a profusion of what we call data. It is no longer difficult to zoom out and see things at a different scale and not difficult to find data that represents some large-scale phenomenon. This could be data from weather stations or tweets about YouTube celebrities. It could be data from shipping routes for trucks or data on the number of avocados grown in California last year. Regardless, it's all about the notion that there is a profusion of data available and the growing need to manage this data to make interesting claims. In that sense, data science often overlaps with social science. \n",
    "\n",
    "It is not too difficult to see a line of inquiry moving from social sciences towards more social computing and then directly into statistics and even physics. A key part of this movement between different disciplines is about how we handle things at different scales. An anthropologist might work for decades at a single village and feel they are unqualified to speak about the next village a few kilometres away, whereas a physicist might examine a network of Internet servers one day and a network of friendships the next day, and yet find similarities in both networks. This is not to slight the anthropologist. Their craft might emphasise a level of depth that demands they see things in very contingent and specific ways. The anthropologist might tell rich stories and emhphasise thick description at a very local scale. Nor is it a slight to the physicist, as despite all the interesting differences that get washed out when looking at huge scales, the fact that we still see similarities in networks at the most macro scale (like all web traffic on a given day) is extremely fascinating. Instead, it is merely important to highlight that different claims are made at different scales and require different approaches. The reason this is now important to the social scientist is because we have the ability to see and consider the social world scale at scales that were previously inaccessible. Some of this was inaccessible because of the cost involved in collecting the data. Sometimes these scales are part and parcel of the newer ways of doing things. Texting creates a record in the act of texting, speech does not. You can record and transcribe speech, but it is not quite the same. \n",
    "\n",
    "With a little luck and some sound training, you will be able to operate at these scales as a part of your career. Some of my students have gone to work for many of the large tech companies. They make decisions about things like the Facebook newsfeed, Twitter's search results, cryptocurrency marketing strategies, and YouTube's comment algorithms. Others are working for academia or government asking questions about open data and more practical questions. Some examples include  how to measure the prevalence of trans persons on Tumblr, how does a mental health diagnosis affect online conversations in a support forum, how does coming out affect the structure of online friendship networks, and how to organize unions on social media. The answers to these questions are not simply for our interest but often lead to decisions. The idea about seeing at scale is that it allows us to intervene at that scale. Hopefully, when we are through here, it will be clear that not only should we look at more macro scale work, but consider how our own biases intervene at these scales. If we are lucky our work can make a difference, no matter how small, to the lives and experiences of those around us. Often that means learning how aggregates become dumb mobs or wise crowds, how events seem to coalesce around focal ideas, and how the structure of our coded world makes a difference to how we engage in it. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a world built of code. And to understand it, we have to become familiar with some of that code ourselves. So in this book we are, of course, primarily going to be focusing on programming. But I want you to think that the reason we are doing this is so that you can build your own 'socialscope' so to speak. Some socialscopes are like long exposure photography, checking out the same context over long time scales. Some socialscopes are vast, like trying to take a photo of the whole earth. In either case, what we want is to be able to shape the data we can get into a form that allows us to observe things on scales that we couldn't otherwise. Sometimes then we can test claims in ways that help us be confident in what we are seeing and thus what we are claiming. But first we simply need to be able to know what we are looking at. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to use this book \n",
    "\n",
    "This book is meant for social scientists who want to scale up their skills and move from investigations amenable to everyday experience (discourse analysis, interviewing, focus groups) towards  investigations that occur at larger scales. However, this is not a traditional book in quantitative methodology. Statistics are only going to play a minor part in the first half of the book (and only a supporting role in the rest). Surveys are going to be almost entirely absent. Instead, the book is going to focus most intently on programming skills and how to translate these to meaningful questions using the vast amount of data available, particularly on the web. That being said, this book will infuse this programming with a number of concepts from social sciences, particularly sociology, but also economics, communication, political science and even history. You do not need to be an expert in any (or even one) of these fields. You just need to be interested in the work of those who have already been thinking at scale. \n",
    "\n",
    "This is very different from traditional quantitative textbooks. Those tend to start with a data set that has already been cleaned, outliers removed, labels nicely in place, and everything is nice and rectangular. Honestly, data sets like this tend to be kind of dull. For example, intro books for one non-python statistical package often use data on cars and their mileage. Personally, I don't own a car, don't like using fossil fuels if I can avoid them, and I would much rather look at some memes than automotive data. It is the 2020s; if we want to start adapting to this world, we have to start thinking in it. This means data can be fun, data is often messy, and data is definitely contentious. \n",
    "\n",
    "This book is not a gentle introduction to the Python programming language, but you will not need much Python before starting here. There exist a ton of books for that purpose. What can be done for intro to python books has already been done very well by a host of others. Instead, this book begins with the DataFrame. This is Python's version of the table. It has rows and columns. Both rows and columns can have labels and the entire DataFrame can have a name. We can filter, summarise, group, and merge these tables. In the end, much of what we will be doing is moving between data as it exists for collection and data that is processed in tables for analysis. Below is a brisk summary of each of the sections to give you a sense of where we are headed. \n",
    "\n",
    "### Part 1. Understanding programming.  \n",
    "The 'intermediates' rather than the basics. We start here with a discussion of programming, what it means and how to think about it. We will look at the DataFrames, Python's version of a table. We then look at a host of file formats. If you want to get data into a DataFrame, it is likely going to start in one of these formats or something similar. If you have a little experience with programming (perhaps having dabbled over a weekend or two, taken a short 'intro to programming' or 'intro to python' course) then it will be a much more rewarding section. If you aren't sure what skills you need to get started here, check Appendix xx. There I list all of the concepts I expect you to know before we start. I also give a quick primer and then point to other places for more extensive learning. \n",
    "\n",
    "### Part 2. Exploring and merging data\n",
    "Data from the web was not made for our analysis. It was made for whatever is its core purpose. To convert this data into a form that us useful we often have to clean this data. The first chapter looks at cleaning up text files. The second chapter looks at distributions. Here we will be doing some rudimentary visualisations and statistics. This is practical applied statistics, not statistics theory. We will keep formulate to a minimum but point to places where the reader can learn more. Finally, we have a chapter on merging and grouping data. Often aggregating data or merging two different sources can be the site of some of the most interesting data analysis. We will primarily be using data from Wikipedia and the World Bank to be doing this. \n",
    "\n",
    "### Part 3. Crafting data \n",
    "People have asked, \"why don't you have the research questions at the beginning?\" It's because research questions are actually pretty difficult to get right. I struggle with them to this day. But by this point in the book we have some skills and the ability to merge in data from different sources. Now is the place where we can start to ask research questions that draw upon this data. We can use our simple stats to start thinking about \"differences that make a difference\" and start conceptualising what data sources we want  as well as what data sources we can access. We will cover APIs. If data powers the web, APIs are like the electrical sockets that need to be the right shape and expect the right kind of data to transfer. We will also cover some issues with good data stewardship. Sometimes it is simply best not to collect some data or to consider alternative strategies in order to best respect our population of study. \n",
    "\n",
    "### Part 4. Researching data \n",
    "These are four discrete chapters demonstrating different aspects of data science. Each one is just enough to get you started on your way in that particular genre of research. There's a chapter on natural language processing that explores some of the important tools and features of this field. This includes sentiment analysis and keyword extraction. The next chapter is on machine learning classifiers. This builds off the previous chapter to look at ways in which we can distinguish the text that we have cleaned. The next chapter examines social network analysis, a powerful approach that helps us understand the world like a map of relationships. These relationships might be based on who replies to who's comment, who posts in the same message board or who tends to fight with each other online. Finally, we look at time series analysis. Python has lots of features for slicing up data by time. You might want to know if a person is more active in the winter, whether their comments happen mainly at night, or whether there are bursts of activity on the weekend.\n",
    "\n",
    "### Part 5. Big Data and professional research \n",
    "These final chapters are meant to get you on your way for the next chapter in your learning. By the end of this book you should be able to embark on more advanced texts in computational social science and statistical learning. You might want to go deeper into the techniques of data science or dive into NLP. I first consider some of the tools required to ensure your work is robust (for example by working on a server rather than a personal computer) and then look at some of the challenges of doing really big data research. I then bring it back to the practical side. I provide tips on where to publish and present such work as well as some of the ways in which theories of social science can really reinforce the interpretation and research designs of data science. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rest of this chapter will be about the logic of coding within the social sciences. It's full of helpful analogies and ideas about how to code that have come from over a decade of teaching it to people (who often were very hesitant at first but almost always get the hang of it and find their footing). "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
